{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Brief Description\n\nMy final solution is based on ensemble of LGBM and SAKT, which give 0.786 on private leaderboard, 0.784 on public leaderboard. This notebook is the inference based on the single LGBM."},{"metadata":{"trusted":true},"cell_type":"code","source":"%reset -f","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom random import sample \nfrom collections import defaultdict\nfrom tqdm import tqdm\nimport time\nimport pickle\n\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n#sns.set()\n\nimport gc\nimport riiideducation\n\nimport os\n\ngc.collect()","execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}},{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"4"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"RUN_CONTENT_AGG = False\nRUN_USER_AGG = False\nRUN_ATTEMPT_NO_AGG = False\nDEBUG = False","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"prior_question_elapsed_time_mean = 25423.84 # mean of all train\ndef preprocess_df(path):\n    cols = ['user_id', 'answered_correctly', 'content_id', 'timestamp']\n    df = pd.read_pickle(path)[cols].reset_index(drop=True)\n    df = df[df.answered_correctly != -1].reset_index(drop=True)\n    return df\n\ntrain_path = \"/kaggle/input/riiid-train-data-multiple-formats/riiid_train.pkl.gzip\"\n\ntrain = preprocess_df(train_path)\n\nif DEBUG:\n    train = train[:1000000]\nprint(\"Train size:\", train.shape)\ntrain.head()","execution_count":4,"outputs":[{"output_type":"stream","text":"Train size: (99271300, 4)\n","name":"stdout"},{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"   user_id  answered_correctly  content_id  timestamp\n0      115                   1        5692          0\n1      115                   1        5716      56943\n2      115                   1         128     118363\n3      115                   1        7860     131167\n4      115                   1        7922     137965","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>answered_correctly</th>\n      <th>content_id</th>\n      <th>timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>115</td>\n      <td>1</td>\n      <td>5692</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>115</td>\n      <td>1</td>\n      <td>5716</td>\n      <td>56943</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>115</td>\n      <td>1</td>\n      <td>128</td>\n      <td>118363</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>115</td>\n      <td>1</td>\n      <td>7860</td>\n      <td>131167</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>115</td>\n      <td>1</td>\n      <td>7922</td>\n      <td>137965</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering\n\nGenerate dictionaries to store users's statistics on full train data."},{"metadata":{},"cell_type":"markdown","source":"### dict_user_previous_ts & dict_user_continuous_correct\n**dict_user_previous_ts:** dictionary to record previous 3 answers timestamp  \n**dict_user_continuous_correct:** dictionary to record previous conrinuous correct answers"},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_user_previous_ts = defaultdict(list)\ndict_user_continuous_correct = defaultdict(int)\n\nfor i, row in enumerate(tqdm(train.loc[:50000000, ['user_id', 'timestamp', 'answered_correctly']].values)):\n    if len(dict_user_previous_ts[row[0]]) == 3:\n        dict_user_previous_ts[row[0]].pop(0)\n        dict_user_previous_ts[row[0]].append(row[1])\n    else:\n        dict_user_previous_ts[row[0]].append(row[1])\n        \n    if row[2] == 0:\n        dict_user_continuous_correct[row[0]] = 0\n    else:\n        dict_user_continuous_correct[row[0]] += 1\n\ntime.sleep(15)\n\nfor i, row in enumerate(tqdm(train.loc[50000001:, ['user_id', 'timestamp', 'answered_correctly']].values)):\n    if len(dict_user_previous_ts[row[0]]) == 3:\n        dict_user_previous_ts[row[0]].pop(0)\n        dict_user_previous_ts[row[0]].append(row[1])\n    else:\n        dict_user_previous_ts[row[0]].append(row[1])\n        \n    if row[2] == 0:\n        dict_user_continuous_correct[row[0]] = 0\n    else:\n        dict_user_continuous_correct[row[0]] += 1","execution_count":5,"outputs":[{"output_type":"stream","text":"100%|██████████| 50000001/50000001 [04:12<00:00, 198234.45it/s]\n100%|██████████| 49271299/49271299 [04:11<00:00, 195924.31it/s]\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"### dict_user_content_total_questions  \n\nThis is the dictionary to keep track of (user_id, content_id) pairs. To get how many times the user has seen the question previously. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# with trace(\"create default\"):\n#     dict_user_content_total_questions = defaultdict(lambda: defaultdict(int))\n# with trace(\"run loop\"):    \n#     for i, row in enumerate(train[['user_id', 'content_id']].values):\n#         dict_user_content_total_questions[row[0]][row[1]] += 1\n\nif RUN_ATTEMPT_NO_AGG:\n    train.drop(['timestamp', 'answered_correctly'], axis=1, inplace=True)\n    gc.collect()\n    time.sleep(15)\n    \n    train[\"attempt_no\"] = 1\n    train.attempt_no=train.attempt_no.astype('int8')\n    attempt_no_agg=train.groupby([\"user_id\",\"content_id\"])[\"attempt_no\"].agg(['sum'])\n    attempt_no_agg=attempt_no_agg.astype('int8')\n    attempt_no_agg = attempt_no_agg[attempt_no_agg['sum']>1]\n    attempt_no_agg.to_pickle('attempt_no_agg.pkl')\nelse:\n    del train\n    gc.collect()\n    time.sleep(15)\n    attempt_no_agg = pd.read_pickle('/kaggle/input/riid-budingtanke/attempt_no_agg.pkl')\n\ndict_user_content_total_questions = defaultdict(lambda: defaultdict(int))\n\nfor index, value in zip(list(attempt_no_agg.index), attempt_no_agg['sum']):\n    dict_user_content_total_questions[index[0]][index[1]] = value\n","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del attempt_no_agg\ngc.collect()\ntime.sleep(15)","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### user_agg\nGet user aggregations."},{"metadata":{"trusted":true},"cell_type":"code","source":"# user aggregation\nif RUN_USER_AGG:\n    questions = pd.read_csv('/kaggle/input/riiid-test-answer-prediction/questions.csv')\n    questions['part'] = questions['part'].astype('int8')\n    \n    user_agg = train.groupby('user_id').aggregate(\\\n                                                  {'answered_correctly': ['count', 'sum'], \\\n                                                   'prior_question_elapsed_time': 'sum', \\\n                                                   'prior_question_had_explanation': 'sum'\n                                                  })\n    user_agg.columns = ['user_total_questions', 'user_correct_questions', 'prior_question_elapsed_time_sum', 'prior_question_had_explanation_sum']\n    user_agg['user_correct_questions'] = user_agg['user_correct_questions'].astype(int)\n    user_agg['prior_question_had_explanation_sum'] = user_agg['prior_question_had_explanation_sum'].astype('float32')\n    user_agg.index.name = None\n    user_agg.to_pickle('user_agg.pkl')  \nelse:\n    user_agg = pd.read_pickle('/kaggle/input/riid-budingtanke/user_agg.pkl')\n    \nuser_agg.head()","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"      user_total_questions  user_correct_questions  \\\n115                     46                      32   \n124                     30                       7   \n2746                    19                      11   \n5382                   125                      84   \n8623                   109                      70   \n\n      prior_question_elapsed_time_sum  prior_question_had_explanation_sum  \n115                      9.224228e+05                                 6.0  \n124                      5.704208e+05                                 0.0  \n2746                     3.504238e+05                                11.0  \n5382                     4.495424e+06                               113.0  \n8623                     2.845024e+06                                96.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_total_questions</th>\n      <th>user_correct_questions</th>\n      <th>prior_question_elapsed_time_sum</th>\n      <th>prior_question_had_explanation_sum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>115</th>\n      <td>46</td>\n      <td>32</td>\n      <td>9.224228e+05</td>\n      <td>6.0</td>\n    </tr>\n    <tr>\n      <th>124</th>\n      <td>30</td>\n      <td>7</td>\n      <td>5.704208e+05</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2746</th>\n      <td>19</td>\n      <td>11</td>\n      <td>3.504238e+05</td>\n      <td>11.0</td>\n    </tr>\n    <tr>\n      <th>5382</th>\n      <td>125</td>\n      <td>84</td>\n      <td>4.495424e+06</td>\n      <td>113.0</td>\n    </tr>\n    <tr>\n      <th>8623</th>\n      <td>109</td>\n      <td>70</td>\n      <td>2.845024e+06</td>\n      <td>96.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create dictionaries from user aggregation\ndict_user_total_questions = dict(user_agg['user_total_questions'])\ndict_user_correct_questions = dict(user_agg['user_correct_questions'])\ndict_user_prior_question_elapsed_time_sum = dict(user_agg['prior_question_elapsed_time_sum'])\ndict_user_prior_question_had_explanation_sum = dict(user_agg['prior_question_had_explanation_sum'])\n\ndict_user_total_questions = defaultdict(int, dict_user_total_questions)\ndict_user_correct_questions = defaultdict(int, dict_user_correct_questions)\ndict_user_prior_question_elapsed_time_sum = defaultdict(int, dict_user_prior_question_elapsed_time_sum)\ndict_user_prior_question_had_explanation_sum = defaultdict(int, dict_user_prior_question_had_explanation_sum)\n\ndel user_agg\ngc.collect()","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"20"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### content_agg\nGet content aggregations."},{"metadata":{"trusted":true},"cell_type":"code","source":"# content aggregation\ndef get_content_agg(train):\n    content_agg = train.groupby('content_id').aggregate({'answered_correctly': ['count', np.nanmean], \\\n                                                         'prior_question_elapsed_time': np.nanmean, \\\n                                                         'prior_question_had_explanation': np.nanmean})\n    content_agg.columns = ['content_total_questions', 'content_accuracy', \\\n                           'content_prior_question_elapsed_time_avg', \\\n                           'content_prior_question_had_explanation_avg']\n\n    content_agg = content_agg.merge(questions[['question_id', 'part']], how='left', \\\n                           left_index=True, right_on='question_id')\\\n    .drop('question_id', axis=1)\n    \n    content_explanation_agg = train[[\"content_id\",\"prior_question_had_explanation\",'answered_correctly']].groupby([\"content_id\",\"prior_question_had_explanation\"])['answered_correctly'].agg(['mean'])\n    content_explanation_agg = content_explanation_agg.unstack()\n    content_explanation_agg=content_explanation_agg.reset_index()\n    content_explanation_agg.columns = ['content_id', 'content_explanation_false_mean','content_explanation_true_mean']\n    \n    content_agg = content_agg.merge(content_explanation_agg, how='left', \\\n                                   left_index=True, right_on='content_id'\\\n                                   ).drop('content_id', axis=1)\n\n    column_type = {'content_total_questions':'int64', \\\n                   'content_accuracy': 'float16', \\\n                   'content_prior_question_elapsed_time_avg': 'float32', \\\n                   'content_prior_question_had_explanation_avg': 'float16', \\\n                   'part': 'int8', \\\n                   'content_explanation_false_mean': 'float16', \\\n                   'content_explanation_true_mean': 'float16'}\n    content_agg = content_agg.astype(column_type)\n    \n    content_agg.to_pickle('content_agg.pkl')   \n    return content_agg\n\nif RUN_CONTENT_AGG:\n    content_agg = get_content_agg(train)\n\nelse:\n    # saved content_agg is calculated on all train\n    content_agg = pd.read_pickle('/kaggle/input/riid-budingtanke/content_agg_3.pkl')\n\ncontent_agg = content_agg[['content_total_questions', \\\n                           'content_accuracy',\\\n                           'content_accuracy_std', \\\n                           'content_prior_question_elapsed_time_avg', \\\n                           'content_prior_question_had_explanation_avg', \\\n                           'part', \\\n#                            'bundle_id', \\\n                           'content_explanation_false_mean', \\\n                           'content_explanation_true_mean']]\n\ncontent_agg.head()","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"   content_total_questions  content_accuracy  content_accuracy_std  \\\n0                     6903          0.907715              0.289440   \n1                     7398          0.890625              0.312104   \n2                    44905          0.554199              0.497050   \n3                    22973          0.779297              0.414636   \n4                    31736          0.613281              0.487021   \n\n   content_prior_question_elapsed_time_avg  \\\n0                             21876.357422   \n1                             22091.626953   \n2                             23546.447266   \n3                             23318.945312   \n4                             23126.990234   \n\n   content_prior_question_had_explanation_avg  part  \\\n0                                    0.947754     1   \n1                                    0.980469     1   \n2                                    0.888184     1   \n3                                    0.958496     1   \n4                                    0.530273     1   \n\n   content_explanation_false_mean  content_explanation_true_mean  \n0                        0.830566                       0.912109  \n1                        0.813965                       0.892090  \n2                        0.490967                       0.562012  \n3                        0.686035                       0.783691  \n4                        0.566895                       0.654297  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>content_total_questions</th>\n      <th>content_accuracy</th>\n      <th>content_accuracy_std</th>\n      <th>content_prior_question_elapsed_time_avg</th>\n      <th>content_prior_question_had_explanation_avg</th>\n      <th>part</th>\n      <th>content_explanation_false_mean</th>\n      <th>content_explanation_true_mean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6903</td>\n      <td>0.907715</td>\n      <td>0.289440</td>\n      <td>21876.357422</td>\n      <td>0.947754</td>\n      <td>1</td>\n      <td>0.830566</td>\n      <td>0.912109</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7398</td>\n      <td>0.890625</td>\n      <td>0.312104</td>\n      <td>22091.626953</td>\n      <td>0.980469</td>\n      <td>1</td>\n      <td>0.813965</td>\n      <td>0.892090</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>44905</td>\n      <td>0.554199</td>\n      <td>0.497050</td>\n      <td>23546.447266</td>\n      <td>0.888184</td>\n      <td>1</td>\n      <td>0.490967</td>\n      <td>0.562012</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>22973</td>\n      <td>0.779297</td>\n      <td>0.414636</td>\n      <td>23318.945312</td>\n      <td>0.958496</td>\n      <td>1</td>\n      <td>0.686035</td>\n      <td>0.783691</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>31736</td>\n      <td>0.613281</td>\n      <td>0.487021</td>\n      <td>23126.990234</td>\n      <td>0.530273</td>\n      <td>1</td>\n      <td>0.566895</td>\n      <td>0.654297</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Load Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = lgb.Booster(model_file='/kaggle/input/riid-budingtanke/model-15.txt')","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = 'answered_correctly'\nfeatures = ['prior_question_had_explanation', \\\n            'prior_question_elapsed_time', \\\n            'user_cum_total_questions', \\\n            'user_cum_accuracy', \\\n            'user_cum_prior_question_elapsed_time_avg', \\\n            'user_cum_prior_question_had_explanation_avg', \\\n            'user_cum_content_total_questions', \\\n            'content_total_questions', \\\n            'content_accuracy', \\\n            'content_accuracy_std', \\\n            'content_prior_question_elapsed_time_avg', \\\n            'content_prior_question_had_explanation_avg', \\\n            'content_explanation_false_mean', \\\n            'content_explanation_true_mean', \\\n            'user_ts_lag_1', \\\n            'user_ts_lag_2', \\\n            'user_ts_lag_3', \\\n            'user_continuous_correct', \\\n            'part' \n           ]","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = riiideducation.make_env()","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iter_test = env.iter_test()","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prior_test_df = None\n\nfor (test_df, sample_prediction_df) in iter_test:\n    \n    if prior_test_df is not None:\n        prior_test_df[target] = eval(test_df[\"prior_group_answers_correct\"].iloc[0])\n        prior_test_df = prior_test_df[prior_test_df.content_type_id == 0]\n        \n        ################ update dicts ####################\n        for i, row in enumerate(prior_test_df[['user_id', \\\n                                               'answered_correctly', \\\n                                               'prior_question_elapsed_time', \\\n                                               'prior_question_had_explanation', \\\n                                               'content_id', \\\n                                               'timestamp']].values):\n            \n            dict_user_total_questions[row[0]] += 1\n            dict_user_correct_questions[row[0]] += row[1]\n            dict_user_prior_question_elapsed_time_sum[row[0]] += row[2]\n            dict_user_prior_question_had_explanation_sum[row[0]] += row[3]\n            dict_user_content_total_questions[row[0]][row[4]] += 1\n            \n            if len(dict_user_previous_ts[row[0]]) == 3:\n                dict_user_previous_ts[row[0]].pop(0)\n                dict_user_previous_ts[row[0]].append(row[5])\n            else:\n                dict_user_previous_ts[row[0]].append(row[5])\n                \n            if row[1] == 0:\n                dict_user_continuous_correct[row[0]] = 0\n            else:\n                dict_user_continuous_correct[row[0]] += 1     \n                \n        ############# preprocess test_df #################\n        cols = ['row_id', \\\n                'user_id', \\\n                'content_id', \\\n                'content_type_id', \\\n                'prior_question_had_explanation', \\\n                'prior_question_elapsed_time', \\\n                'timestamp']\n        test_df = test_df[cols]\n        test_df['prior_question_had_explanation'] = test_df['prior_question_had_explanation'].fillna(0).astype('bool')\n        test_df['prior_question_elapsed_time'] = test_df['prior_question_elapsed_time'].fillna(prior_question_elapsed_time_mean)\n\n        prior_test_df = test_df.copy()\n        \n        ############# get features ################\n        user_cum_total_questions = np.zeros(test_df.shape[0], dtype=np.int32)\n        user_cum_accuracy = np.zeros(test_df.shape[0], dtype=np.float32)\n        user_cum_prior_question_elapsed_time_avg = np.zeros(test_df.shape[0], dtype=np.float32)\n        user_cum_prior_question_had_explanation_avg = np.zeros(test_df.shape[0], dtype=np.float32)\n        user_cum_content_total_questions = np.zeros(test_df.shape[0], dtype=np.int32)\n        user_ts_lag_1 = np.zeros(test_df.shape[0], dtype = np.float32)\n        user_ts_lag_2 = np.zeros(test_df.shape[0], dtype = np.float32)\n        user_ts_lag_3 = np.zeros(test_df.shape[0], dtype = np.float32)\n        user_continuous_correct = np.zeros(test_df.shape[0], dtype = np.float16)\n\n        for i, row in enumerate(tqdm(test_df[['user_id', \\\n                                         'prior_question_elapsed_time', \\\n                                         'prior_question_had_explanation', \\\n                                         'content_id', \\\n                                         'timestamp']].values)):\n            # get features from dicts\n            user_cum_total_questions[i] = dict_user_total_questions[row[0]]\n            if dict_user_total_questions[row[0]] != 0:      \n                user_cum_accuracy[i] = dict_user_correct_questions[row[0]]/dict_user_total_questions[row[0]]\n                user_cum_prior_question_elapsed_time_avg[i] = dict_user_prior_question_elapsed_time_sum[row[0]]/dict_user_total_questions[row[0]]\n                user_cum_prior_question_had_explanation_avg[i] = dict_user_prior_question_had_explanation_sum[row[0]]/dict_user_total_questions[row[0]]\n            else:\n                user_cum_accuracy[i] = np.nan \n                user_cum_prior_question_elapsed_time_avg[i] = np.nan \n                user_cum_prior_question_had_explanation_avg[i] = np.nan\n\n            user_cum_content_total_questions[i] = dict_user_content_total_questions[row[0]][row[3]]\n\n            if len(dict_user_previous_ts[row[0]]) == 0:\n                user_ts_lag_1[i] = np.nan\n                user_ts_lag_2[i] = np.nan\n                user_ts_lag_3[i] = np.nan\n            elif len(dict_user_previous_ts[row[0]]) == 1:\n                user_ts_lag_1[i] = row[4] - dict_user_previous_ts[row[0]][0]\n                user_ts_lag_2[i] = np.nan\n                user_ts_lag_3[i] = np.nan\n            elif len(dict_user_previous_ts[row[0]]) == 2:\n                user_ts_lag_1[i] = row[4] - dict_user_previous_ts[row[0]][1]\n                user_ts_lag_2[i] = row[4] - dict_user_previous_ts[row[0]][0]\n                user_ts_lag_3[i] = np.nan\n            elif len(dict_user_previous_ts[row[0]]) == 3:\n                user_ts_lag_1[i] = row[4] - dict_user_previous_ts[row[0]][2]\n                user_ts_lag_2[i] = row[4] - dict_user_previous_ts[row[0]][1]\n                user_ts_lag_3[i] = row[4] - dict_user_previous_ts[row[0]][0]\n            \n            if row[4] == 0:\n                user_continuous_correct[i] = np.nan\n            else:\n                user_continuous_correct[i] = dict_user_continuous_correct[row[0]]\n                \n        # add new features to df\n        test_df['user_cum_total_questions'] = user_cum_total_questions\n        test_df['user_cum_accuracy'] = user_cum_accuracy\n        test_df['user_cum_prior_question_elapsed_time_avg'] = user_cum_prior_question_elapsed_time_avg\n        test_df['user_cum_prior_question_had_explanation_avg'] = user_cum_prior_question_had_explanation_avg\n        test_df['user_cum_content_total_questions'] = user_cum_content_total_questions\n        test_df['user_ts_lag_1'] = user_ts_lag_1\n        test_df['user_ts_lag_2'] = user_ts_lag_2\n        test_df['user_ts_lag_3'] = user_ts_lag_3\n        test_df['user_continuous_correct'] = user_continuous_correct\n        \n        test_df = test_df.merge(content_agg, right_index=True, left_on='content_id', how='left')\n        test_df['answered_correctly'] =  model.predict(test_df[features])\n        \n    else:\n        ############# preprocess test_df #################\n        cols = ['row_id', \\\n                'user_id', \\\n                'content_id', \\\n                'content_type_id', \\\n                'prior_question_had_explanation', \\\n                'prior_question_elapsed_time', \\\n                'timestamp']\n        test_df = test_df[cols]\n        test_df['prior_question_had_explanation'] = test_df['prior_question_had_explanation'].fillna(0).astype('bool')\n        test_df['prior_question_elapsed_time'] = test_df['prior_question_elapsed_time'].fillna(prior_question_elapsed_time_mean)\n\n        prior_test_df = test_df.copy()\n        \n        ############# get features ################\n        user_cum_total_questions = np.zeros(test_df.shape[0], dtype=np.int32)\n        user_cum_accuracy = np.zeros(test_df.shape[0], dtype=np.float32)\n        user_cum_prior_question_elapsed_time_avg = np.zeros(test_df.shape[0], dtype=np.float32)\n        user_cum_prior_question_had_explanation_avg = np.zeros(test_df.shape[0], dtype=np.float32)\n        user_cum_content_total_questions = np.zeros(test_df.shape[0], dtype=np.int32)\n        user_ts_lag_1 = np.zeros(test_df.shape[0], dtype = np.float32)\n        user_ts_lag_2 = np.zeros(test_df.shape[0], dtype = np.float32)\n        user_ts_lag_3 = np.zeros(test_df.shape[0], dtype = np.float32)\n        user_continuous_correct = np.zeros(test_df.shape[0], dtype = np.float16)\n\n        for i, row in enumerate(tqdm(test_df[['user_id', \\\n                                         'prior_question_elapsed_time', \\\n                                         'prior_question_had_explanation', \\\n                                         'content_id', \\\n                                         'timestamp']].values)):\n            # get features from dicts\n            user_cum_total_questions[i] = dict_user_total_questions[row[0]]\n            if dict_user_total_questions[row[0]] != 0:      \n                user_cum_accuracy[i] = dict_user_correct_questions[row[0]]/dict_user_total_questions[row[0]]\n                user_cum_prior_question_elapsed_time_avg[i] = dict_user_prior_question_elapsed_time_sum[row[0]]/dict_user_total_questions[row[0]]\n                user_cum_prior_question_had_explanation_avg[i] = dict_user_prior_question_had_explanation_sum[row[0]]/dict_user_total_questions[row[0]]\n            else:\n                user_cum_accuracy[i] = np.nan \n                user_cum_prior_question_elapsed_time_avg[i] = np.nan \n                user_cum_prior_question_had_explanation_avg[i] = np.nan\n\n            user_cum_content_total_questions[i] = dict_user_content_total_questions[row[0]][row[3]]\n\n            if len(dict_user_previous_ts[row[0]]) == 0:\n                user_ts_lag_1[i] = np.nan\n                user_ts_lag_2[i] = np.nan\n                user_ts_lag_3[i] = np.nan\n            elif len(dict_user_previous_ts[row[0]]) == 1:\n                user_ts_lag_1[i] = row[4] - dict_user_previous_ts[row[0]][0]\n                user_ts_lag_2[i] = np.nan\n                user_ts_lag_3[i] = np.nan\n            elif len(dict_user_previous_ts[row[0]]) == 2:\n                user_ts_lag_1[i] = row[4] - dict_user_previous_ts[row[0]][1]\n                user_ts_lag_2[i] = row[4] - dict_user_previous_ts[row[0]][0]\n                user_ts_lag_3[i] = np.nan\n            elif len(dict_user_previous_ts[row[0]]) == 3:\n                user_ts_lag_1[i] = row[4] - dict_user_previous_ts[row[0]][2]\n                user_ts_lag_2[i] = row[4] - dict_user_previous_ts[row[0]][1]\n                user_ts_lag_3[i] = row[4] - dict_user_previous_ts[row[0]][0]\n            \n            if row[4] == 0:\n                user_continuous_correct[i] = np.nan\n            else:\n                user_continuous_correct[i] = dict_user_continuous_correct[row[0]]\n                \n        # add new features to df\n        test_df['user_cum_total_questions'] = user_cum_total_questions\n        test_df['user_cum_accuracy'] = user_cum_accuracy\n        test_df['user_cum_prior_question_elapsed_time_avg'] = user_cum_prior_question_elapsed_time_avg\n        test_df['user_cum_prior_question_had_explanation_avg'] = user_cum_prior_question_had_explanation_avg\n        test_df['user_cum_content_total_questions'] = user_cum_content_total_questions\n        test_df['user_ts_lag_1'] = user_ts_lag_1\n        test_df['user_ts_lag_2'] = user_ts_lag_2\n        test_df['user_ts_lag_3'] = user_ts_lag_3\n        test_df['user_continuous_correct'] = user_continuous_correct\n        \n        test_df = test_df.merge(content_agg, right_index=True, left_on='content_id', how='left')\n        test_df['answered_correctly'] =  model.predict(test_df[features])\n    \n    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])","execution_count":15,"outputs":[{"output_type":"stream","text":"100%|██████████| 18/18 [00:00<00:00, 3039.35it/s]\n100%|██████████| 27/27 [00:00<00:00, 19819.08it/s]\n100%|██████████| 26/26 [00:00<00:00, 18331.13it/s]\n100%|██████████| 33/33 [00:00<00:00, 21956.22it/s]\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"# Draft"},{"metadata":{"trusted":true},"cell_type":"code","source":"# cols = ['user_id', 'answered_correctly', 'content_id', 'content_type_id', \\\n#         'prior_question_had_explanation', 'prior_question_elapsed_time', 'timestamp']\n# target_df = pd.read_pickle('../input/riiid-cross-validation-files/cv1_train.pickle')[50_000_000:52_500_000]#[cols]\n\n# target_df = preprocess_test_df(target_df)","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# class Iter_Valid(object):\n#     def __init__(self, df, max_user=1000):\n#         df = df.reset_index(drop=True)\n#         self.df = df\n#         self.user_answer = df['user_answer'].astype(str).values\n#         self.answered_correctly = df['answered_correctly'].astype(str).values\n#         df['prior_group_responses'] = \"[]\"\n#         df['prior_group_answers_correct'] = \"[]\"\n#         self.sample_df = df[df['content_type_id'] == 0][['row_id']]\n#         self.sample_df['answered_correctly'] = 0\n#         self.len = len(df)\n#         self.user_id = df.user_id.values\n#         self.task_container_id = df.task_container_id.values\n#         self.content_type_id = df.content_type_id.values\n#         self.max_user = max_user\n#         self.current = 0\n#         self.pre_user_answer_list = []\n#         self.pre_answered_correctly_list = []\n\n#     def __iter__(self):\n#         return self\n    \n#     def fix_df(self, user_answer_list, answered_correctly_list, pre_start):\n#         df= self.df[pre_start:self.current].copy()\n#         sample_df = self.sample_df[pre_start:self.current].copy()\n#         df.loc[pre_start,'prior_group_responses'] = '[' + \",\".join(self.pre_user_answer_list) + ']'\n#         df.loc[pre_start,'prior_group_answers_correct'] = '[' + \",\".join(self.pre_answered_correctly_list) + ']'\n#         self.pre_user_answer_list = user_answer_list\n#         self.pre_answered_correctly_list = answered_correctly_list\n#         return df, sample_df\n\n#     def __next__(self):\n#         added_user = set()\n#         pre_start = self.current\n#         pre_added_user = -1\n#         pre_task_container_id = -1\n\n#         user_answer_list = []\n#         answered_correctly_list = []\n#         while self.current < self.len:\n#             crr_user_id = self.user_id[self.current]\n#             crr_task_container_id = self.task_container_id[self.current]\n#             crr_content_type_id = self.content_type_id[self.current]\n#             if crr_content_type_id == 1:\n#                 # no more than one task_container_id of \"questions\" from any single user\n#                 # so we only care for content_type_id == 0 to break loop\n#                 user_answer_list.append(self.user_answer[self.current])\n#                 answered_correctly_list.append(self.answered_correctly[self.current])\n#                 self.current += 1\n#                 continue\n#             if crr_user_id in added_user and ((crr_user_id != pre_added_user) or (crr_task_container_id != pre_task_container_id)):\n#                 # known user(not prev user or differnt task container)\n#                 return self.fix_df(user_answer_list, answered_correctly_list, pre_start)\n#             if len(added_user) == self.max_user:\n#                 if  crr_user_id == pre_added_user and crr_task_container_id == pre_task_container_id:\n#                     user_answer_list.append(self.user_answer[self.current])\n#                     answered_correctly_list.append(self.answered_correctly[self.current])\n#                     self.current += 1\n#                     continue\n#                 else:\n#                     return self.fix_df(user_answer_list, answered_correctly_list, pre_start)\n#             added_user.add(crr_user_id)\n#             pre_added_user = crr_user_id\n#             pre_task_container_id = crr_task_container_id\n#             user_answer_list.append(self.user_answer[self.current])\n#             answered_correctly_list.append(self.answered_correctly[self.current])\n#             self.current += 1\n#         if pre_start < self.current:\n#             return self.fix_df(user_answer_list, answered_correctly_list, pre_start)\n#         else:\n#             raise StopIteration()","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# iter_test = Iter_Valid(target_df,max_user=1000)\n# predicted = []\n# def set_predict(df):\n#     predicted.append(df)","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pbar = tqdm(total=2500000)\n# prior_test_df = None\n\n# for (test_df, sample_prediction_df) in iter_test:\n    \n#     if prior_test_df is not None:\n#         prior_test_df[target] = eval(test_df[\"prior_group_answers_correct\"].iloc[0])\n#         prior_test_df = prior_test_df[prior_test_df.content_type_id == 0]\n    \n#         for i, row in enumerate(prior_test_df[['user_id', \\\n#                                                'answered_correctly', \\\n#                                                'prior_question_elapsed_time', \\\n#                                                'prior_question_had_explanation', \\\n#                                                'content_id', \\\n#                                                'timestamp']].values):\n#             dict_user_total_questions[row[0]] += 1\n#             dict_user_correct_questions[row[0]] += row[1]\n#             dict_user_prior_question_elapsed_time_sum[row[0]] += row[2]\n#             dict_user_prior_question_had_explanation_sum[row[0]] += row[3]\n#             dict_user_content_total_questions[row[0]][row[4]] += 1\n            \n#             if len(dict_user_previous_ts[row[0]]) == 3:\n#                 dict_user_previous_ts[row[0]].pop(0)\n#                 dict_user_previous_ts[row[0]].append(row[5])\n#             else:\n#                 dict_user_previous_ts[row[0]].append(row[5])\n\n#         test_df = preprocess_test_df(test_df)\n#         prior_test_df = test_df.copy()\n#         test_df = get_features(test_df, \\\n#                                dict_user_total_questions, \\\n#                                dict_user_correct_questions, \\\n#                                dict_user_prior_question_elapsed_time_sum, \\\n#                                dict_user_prior_question_had_explanation_sum, \\\n#                                dict_user_content_total_questions, \\\n#                                dict_user_previous_ts)\n#         test_df = test_df.merge(content_agg, right_index=True, left_on='content_id', how='left')\n#         test_df['answered_correctly'] =  model.predict(test_df[features])\n        \n#     else:\n        \n#         test_df = preprocess_test_df(test_df)\n#         prior_test_df = test_df.copy()\n#         test_df = test_df[test_df.content_type_id == 0]\n#         test_df = get_features(test_df, \\\n#                                dict_user_total_questions, \\\n#                                dict_user_correct_questions, \\\n#                                dict_user_prior_question_elapsed_time_sum, \\\n#                                dict_user_prior_question_had_explanation_sum, \\\n#                                dict_user_content_total_questions, \\\n#                                dict_user_previous_ts)\n#         test_df = test_df.merge(content_agg, right_index=True, left_on='content_id', how='left')\n#         test_df['answered_correctly'] =  model.predict(test_df[features])\n    \n#     test_df = test_df[test_df.content_type_id == 0]\n#     set_predict(test_df.loc[:,['row_id', 'answered_correctly']])\n#     pbar.update(len(test_df))","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}